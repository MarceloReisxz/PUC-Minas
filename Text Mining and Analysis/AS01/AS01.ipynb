{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Marcelo Reis Esteves - AS01\n",
        "---"
      ],
      "metadata": {
        "id": "-HD5vo3tqyQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Carregando conteúdo Shakespeare.txt\n",
        "url = \"https://raw.githubusercontent.com/MarceloReisxz/PUC-Minas/refs/heads/main/Text%20Mining%20and%20Analysis/AS01/Shakespeare.txt\"\n",
        "\n",
        "response = requests.get(url)\n",
        "text = response.text"
      ],
      "metadata": {
        "id": "yrcNvewPs1e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StRBiIRQgUZT"
      },
      "source": [
        "## Normalização\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go8g40RrgrxO"
      },
      "source": [
        "# Lower case reduction\n",
        "texto_normalizado = text.lower()\n",
        "\n",
        "# ----------------------------------------\n",
        "\n",
        "# Accent and diacritic removal\n",
        "import unicodedata\n",
        "\n",
        "def remove_accents(input_str):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    return u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
        "\n",
        "texto_normalizado = remove_accents(texto_normalizado)\n",
        "\n",
        "# ----------------------------------------\n",
        "\n",
        "# Canonicalizing of acronyms, currency, date and hyphenated words\n",
        "import re\n",
        "\n",
        "# acronyms\n",
        "texto_normalizado = re.sub(r'\\.(?!([^.\\s]|\\d))', '', texto_normalizado)\n",
        "\n",
        "# currency\n",
        "text = re.sub(r'(\\$|€|£|r\\$)\\s*(\\d+[\\.,]?\\d*)', r'\\1\\2', text)\n",
        "\n",
        "# date\n",
        "text = re.sub(r'(\\d{1,2})[\\/\\-](\\d{1,2})[\\/\\-](\\d{2,4})', r'\\1-\\2-\\3', text)\n",
        "\n",
        "# hyphenated words\n",
        "text = re.sub(r'(?<=\\b\\w)-(?=\\w\\b)', '', text)\n",
        "\n",
        "# ----------------------------------------\n",
        "\n",
        "# Punctuation removal (except currency and date)\n",
        "texto_normalizado = re.sub(r'(?<!\\d)[.,:;!?\\'\\\"()\\-=](?!\\d)', '', texto_normalizado)\n",
        "\n",
        "# ----------------------------------------\n",
        "\n",
        "# Special characters removal\n",
        "texto_normalizado = re.sub(r'(?<!\\d)[.,:;!?\\'\\\"()\\-=](?!\\d)', '', texto_normalizado)\n",
        "texto_normalizado = re.sub(r' +', ' ', texto_normalizado)\n",
        "texto_normalizado\n",
        "# ----------------------------------------\n",
        "\n",
        "with open(\"Shakespeare_Normalized.txt\", \"w\", encoding=\"utf-8\") as arquivo:\n",
        "    arquivo.write(texto_normalizado)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SspcGgM3qTgI"
      },
      "source": [
        "# Tokenização\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "White Space Tokenization"
      ],
      "metadata": {
        "id": "Vtp7OPGC4lQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_white_space = texto_normalizado.split()\n",
        "with open(\"Shakespeare_Normalized_Tokenized01.txt\", \"w\", encoding=\"utf-8\") as arquivo:\n",
        "    for token in texto_white_space:\n",
        "        arquivo.write(token + \"\\n\")"
      ],
      "metadata": {
        "id": "1HOqG_f-4i6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK: Word Tokenizer"
      ],
      "metadata": {
        "id": "kPl43lAw5RDd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvVoeA2fgE07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6099b63c-fc15-4ef4-e644-4ea9f903805f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import nltk.tokenize as to\n",
        "\n",
        "tokeized_text = to.word_tokenize(texto_normalizado)\n",
        "with open(\"Shakespeare_Normalized_Tokenized02.txt\", \"w\", encoding=\"utf-8\") as arquivo:\n",
        "    for token in tokeized_text:\n",
        "        arquivo.write(token + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK: Tree Bank Tokenizer"
      ],
      "metadata": {
        "id": "ZpMR3DQ_DNI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = to.TreebankWordTokenizer().tokenize(texto_normalizado)\n",
        "with open(\"Shakespeare_Normalized_Tokenized03.txt\", \"w\", encoding=\"utf-8\") as arquivo:\n",
        "    for token in tokenized_text:\n",
        "        arquivo.write(token + \"\\n\")"
      ],
      "metadata": {
        "id": "l0V8Zl4xDMzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK: Word Punctuation Tokenizer"
      ],
      "metadata": {
        "id": "Sj-7nda4DN4J"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg6YZzL_wRsE"
      },
      "source": [
        "tokenized_text = to.WordPunctTokenizer().tokenize(texto_normalizado)\n",
        "with open(\"Shakespeare_Normalized_Tokenized04.txt\", \"w\", encoding=\"utf-8\") as arquivo:\n",
        "    for token in tokenized_text:\n",
        "        arquivo.write(token + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK: Tweet Tokenizer"
      ],
      "metadata": {
        "id": "9rsjvLNuDnAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = to.TweetTokenizer().tokenize(texto_normalizado)\n",
        "with open(\"Shakespeare_Normalized_Tokenized05.txt\", \"w\", encoding=\"utf-8\") as arquivo:\n",
        "    for token in tokenized_text:\n",
        "        arquivo.write(token + \"\\n\")"
      ],
      "metadata": {
        "id": "ZsFUj9KbDnsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK: MWE Tokenizer"
      ],
      "metadata": {
        "id": "LQj61FA6Dsbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = to.MWETokenizer().tokenize(texto_normalizado)\n",
        "with open(\"Shakespeare_Normalized_Tokenized06.txt\", \"w\", encoding=\"utf-8\") as arquivo:\n",
        "    for token in tokenized_text:\n",
        "        arquivo.write(token + \"\\n\")"
      ],
      "metadata": {
        "id": "eLYjtxhYDs6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextBlob Word Tokenizer"
      ],
      "metadata": {
        "id": "OVh7jTzPFVZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob\n",
        "from textblob import TextBlob\n",
        "\n",
        "blob_object = TextBlob(texto_normalizado)\n",
        "tokenized_text = blob_object.words\n",
        "with open(\"Shakespeare_Normalized_Tokenized07.txt\", \"w\", encoding=\"utf-8\") as arquivo:\n",
        "    for token in tokenized_text:\n",
        "        arquivo.write(token + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fpe3xrdOFVM8",
        "outputId": "61554d3a-445f-47a9-a3b7-3fb04b3b099b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy Tokenizer"
      ],
      "metadata": {
        "id": "MzGKlIqfGwyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "tokenized_text = nlp(texto_normalizado)\n",
        "\n",
        "with open(\"Shakespeare_Normalized_Tokenized08.txt\", \"w\", encoding=\"utf-8\") as arquivo:\n",
        "    for token in tokenized_text:\n",
        "        arquivo.write(str(token) + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wuFhs3QGv2O",
        "outputId": "233cc1e9-9b5f-46e8-e234-c9aa9b64f69b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gensim Word Tokenizer"
      ],
      "metadata": {
        "id": "ogMt6V9TOQ10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.utils import tokenize\n",
        "\n",
        "tokenized_text = tokenize(texto_normalizado)\n",
        "\n",
        "with open(\"Shakespeare_Normalized_Tokenized09.txt\", \"w\", encoding=\"utf-8\") as arquivo:\n",
        "    for token in tokenized_text:\n",
        "        arquivo.write(token + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxCtzJQtOQoE",
        "outputId": "dd2918a8-8b9a-42ac-e6ce-adfc2f90e802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras Tokenization"
      ],
      "metadata": {
        "id": "t4byjKm2O5xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "tokenized_text = text_to_word_sequence(texto_normalizado)\n",
        "\n",
        "with open(\"Shakespeare_Normalized_Tokenized10.txt\", \"w\", encoding=\"utf-8\") as arquivo:\n",
        "    for token in tokenized_text:\n",
        "        arquivo.write(token + \"\\n\")"
      ],
      "metadata": {
        "id": "a4TQjrzKO5l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eJWkEnIgFf_"
      },
      "source": [
        "## Stop-words\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyBs9FAcgWLb",
        "outputId": "06a0d383-8104-44a8-d624-284f8e35d1bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "arquivo_path = \"Shakespeare_Normalized_Tokenized02.txt\"\n",
        "tokenized_text = []\n",
        "\n",
        "with open(arquivo_path, \"r\", encoding=\"utf-8\") as arquivo:\n",
        "    tokenized_text = arquivo.readlines()\n",
        "\n",
        "tokenized_text = [linha.strip() for linha in tokenized_text]\n",
        "\n",
        "# ----------------------------------------\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "sw = stopwords.words('english')\n",
        "nonstopwords_text = [word for word in tokenized_text if not word in sw]\n",
        "\n",
        "# ----------------------------------------\n",
        "\n",
        "with open(\"Shakespeare_Normalized_Tokenized_StopWord.txt\", \"w\", encoding=\"utf-8\") as arquivo:\n",
        "    for token in nonstopwords_text:\n",
        "        arquivo.write(token + \"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lematização\n",
        "---"
      ],
      "metadata": {
        "id": "_abc2QDRzBzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_text = [lemmatizer.lemmatize(word) for word in nonstopwords_text]\n",
        "\n",
        "with open(\"Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt\", \"w\", encoding=\"utf-8\") as arquivo:\n",
        "    for token in lemmatized_text:\n",
        "        arquivo.write(token + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDXq0P-ZzBlB",
        "outputId": "432e9790-d74f-4fa4-b015-6e71bd8d1f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FOA_3P1gqH-"
      },
      "source": [
        "# Stemming\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Porter Stemmer"
      ],
      "metadata": {
        "id": "0SOv9YP40H6a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQYr-MpAwJtG"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "tokens = [stemmer.stem(token) for token in lemmatized_text]\n",
        "\n",
        "with open(\"Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming01.txt\", \"w\", encoding=\"utf-8\") as arquivo:\n",
        "    for token in tokens:\n",
        "        arquivo.write(token + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Snowball Stemmer"
      ],
      "metadata": {
        "id": "IBi7Uiun0ly7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "tokens = [stemmer.stem(token) for token in lemmatized_text]\n",
        "\n",
        "with open(\"Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming02.txt\", \"w\", encoding=\"utf-8\") as arquivo:\n",
        "    for token in tokens:\n",
        "        arquivo.write(token + \"\\n\")"
      ],
      "metadata": {
        "id": "RPHckSqj0mIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSV Lematizador\n",
        "---"
      ],
      "metadata": {
        "id": "3w0-0K4P9QQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "input_file = \"Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt\"\n",
        "output_file = \"Shakespeare_Vocabulary_Lemmatized.csv\"\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
        "    tokens = [line.strip() for line in file if line.strip()]\n",
        "\n",
        "# Contar a frequência de cada token\n",
        "token_counts = Counter(tokens)\n",
        "\n",
        "data = [(token, count, len(token)) for token, count in token_counts.items()]\n",
        "\n",
        "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Token\", \"Número de Ocorrências\", \"Tamanho (Caracteres)\"])\n",
        "    writer.writerows(data)\n",
        "\n",
        "print(f\"Arquivo CSV '{output_file}' gerado com sucesso!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Zw7MFWg9QBC",
        "outputId": "8722c9ee-0f32-452c-ef9a-a6b47df21c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo CSV 'Shakespeare_Vocabulary_Lemmatized.csv' gerado com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSV Porter Stemmer\n",
        "---"
      ],
      "metadata": {
        "id": "X8ldbb7u91wB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming01.txt\"\n",
        "output_file = \"Shakespeare_Vocabulary_Porter.csv\"\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
        "    tokens = [line.strip() for line in file if line.strip()]\n",
        "\n",
        "# Contar a frequência de cada token\n",
        "token_counts = Counter(tokens)\n",
        "\n",
        "data = [(token, count, len(token)) for token, count in token_counts.items()]\n",
        "\n",
        "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Token\", \"Número de Ocorrências\", \"Tamanho (Caracteres)\"])\n",
        "    writer.writerows(data)\n",
        "\n",
        "print(f\"Arquivo CSV '{output_file}' gerado com sucesso!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef85fd9-c264-408d-9526-6bfca5d9da94",
        "id": "V5571y5j91wC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo CSV 'Shakespeare_Vocabulary_Porter.csv' gerado com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSV Snowball Stemmer\n",
        "---"
      ],
      "metadata": {
        "id": "tBkdKz8692Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming02.txt\"\n",
        "output_file = \"Shakespeare_Vocabulary_Snow.csv\"\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
        "    tokens = [line.strip() for line in file if line.strip()]\n",
        "\n",
        "# Contar a frequência de cada token\n",
        "token_counts = Counter(tokens)\n",
        "\n",
        "data = [(token, count, len(token)) for token, count in token_counts.items()]\n",
        "\n",
        "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Token\", \"Número de Ocorrências\", \"Tamanho (Caracteres)\"])\n",
        "    writer.writerows(data)\n",
        "\n",
        "print(f\"Arquivo CSV '{output_file}' gerado com sucesso!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c039a56-3a11-4199-de5e-bf0fccea9381",
        "id": "wMlESoXj92Mc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo CSV 'Shakespeare_Vocabulary_Snow.csv' gerado com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Txt Comparativo\n",
        "---"
      ],
      "metadata": {
        "id": "8a4nyvnr-5sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Lista de arquivos de entrada e seus respectivos títulos\n",
        "files = [\n",
        "    (\"Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt\", \"LEMATIZADOR (WordNet Lemmatizer)\"),\n",
        "    (\"Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming01.txt\", \"STEMMING (Porter Stemmer)\"),\n",
        "    (\"Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming02.txt\", \"STEMMING (Snowball Stemmer)\")\n",
        "]\n",
        "\n",
        "output_txt = \"Shakespeare_Vocabulary_Analysis.txt\"\n",
        "\n",
        "with open(output_txt, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(\"\")\n",
        "\n",
        "for input_file, title in files:\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
        "        tokens = [line.strip() for line in file if line.strip()]\n",
        "\n",
        "    # Contar a frequência de cada token\n",
        "    token_counts = Counter(tokens)\n",
        "\n",
        "    total_tokens = len(token_counts)\n",
        "    avg_occurrences = sum(token_counts.values()) / total_tokens\n",
        "    avg_length = sum(len(token) for token in token_counts) / total_tokens\n",
        "\n",
        "    with open(output_txt, \"a\", encoding=\"utf-8\") as file:\n",
        "        file.write(f\"---- {title} ------\\n\")\n",
        "        file.write(f\"Tamanho do vocabulário: {total_tokens}\\n\")\n",
        "        file.write(f\"Número médio de ocorrências: {avg_occurrences:.2f}\\n\")\n",
        "        file.write(f\"Tamanho médio dos tokens: {avg_length:.2f}\\n\")\n",
        "        file.write(\"\\n\")\n",
        "\n",
        "print(f\"Arquivo '{output_txt}' gerado com sucesso!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4FciHRb-5eC",
        "outputId": "485a7118-082e-40a9-bb19-a46b6c7aef7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo 'Shakespeare_Vocabulary_Analysis.txt' gerado com sucesso!\n"
          ]
        }
      ]
    }
  ]
}